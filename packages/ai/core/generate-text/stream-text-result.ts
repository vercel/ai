import { ServerResponse } from 'node:http';
import { InferUIMessageStreamPart } from '../../src/ui-message-stream/ui-message-stream-parts';
import { InferUIMessageMetadata, UIMessage } from '../../src/ui/ui-messages';
import { AsyncIterableStream } from '../../src/util/async-iterable-stream';
import { ReasoningPart } from '../prompt/content-part';
import {
  CallWarning,
  FinishReason,
  LanguageModelRequestMetadata,
  ProviderMetadata,
} from '../types';
import { Source } from '../types/language-model';
import { LanguageModelResponseMetadata } from '../types/language-model-response-metadata';
import { LanguageModelUsage } from '../types/usage';
import { ContentPart } from './content-part';
import { GeneratedFile } from './generated-file';
import { ResponseMessage } from './response-message';
import { StepResult } from './step-result';
import { ToolCallUnion } from './tool-call';
import { ToolResultUnion } from './tool-result';
import { ToolSet } from './tool-set';
import { UIMessageStreamResponseInit } from '../../src/ui-message-stream/ui-message-stream-response-init';

export type UIMessageStreamOptions<UI_MESSAGE extends UIMessage> = {
  /**
   * The original messages. If they are provided, persistence mode is assumed,
   * and a message ID is provided for the response message.
   */
  originalMessages?: UI_MESSAGE[];

  onFinish?: (options: {
    /**
     * The updates list of UI messages.
     */
    messages: UI_MESSAGE[];

    /**
     * Indicates whether the response message is a continuation of the last original message,
     * or if a new message was created.
     */
    isContinuation: boolean;

    /**
     * The message that was sent to the client as a response
     * (including the original message if it was extended).
     */
    responseMessage: UI_MESSAGE;
  }) => void;

  /**
   * Extracts message metadata that will be send to the client.
   *
   * Called on `start` and `finish` events.
   */
  messageMetadata?: (options: {
    part: TextStreamPart<ToolSet>;
  }) => InferUIMessageMetadata<UI_MESSAGE> | undefined;

  /**
   * Send reasoning parts to the client.
   * Default to true.
   */
  sendReasoning?: boolean;

  /**
   * Send source parts to the client.
   * Default to false.
   */
  sendSources?: boolean;

  /**
   * Send the finish event to the client.
   * Set to false if you are using additional streamText calls
   * that send additional data.
   * Default to true.
   */
  sendFinish?: boolean;

  /**
   * Send the message start event to the client.
   * Set to false if you are using additional streamText calls
   * and the message start event has already been sent.
   * Default to true.
   *
   * Note: this setting is currently not used, but you should
   * already set it to false if you are using additional
   * streamText calls that send additional data to prevent
   * the message start event from being sent multiple times.
   */
  sendStart?: boolean;

  /**
   * Process an error, e.g. to log it. Default to `() => 'An error occurred.'`.
   *
   * @return error message to include in the data stream.
   */
  onError?: (error: unknown) => string;
};

export type ConsumeStreamOptions = {
  onError?: (error: unknown) => void;
};

/**
A result object for accessing different stream types and additional information.
 */
export interface StreamTextResult<TOOLS extends ToolSet, PARTIAL_OUTPUT> {
  /**
The content that was generated in the last step.

Resolved when the response is finished.
   */
  readonly content: Promise<Array<ContentPart<TOOLS>>>;

  /**
The full text that has been generated by the last step.

Resolved when the response is finished.
     */
  readonly text: Promise<string>;

  /**
The full reasoning that the model has generated.

Resolved when the response is finished.
   */
  readonly reasoning: Promise<Array<ReasoningPart>>;

  /**
The reasoning that has been generated by the last step.

Resolved when the response is finished.
     */
  readonly reasoningText: Promise<string | undefined>;

  /**
Files that have been generated by the model in the last step.

Resolved when the response is finished.
   */
  readonly files: Promise<GeneratedFile[]>;

  /**
Sources that have been used as references in the last step.

Resolved when the response is finished.
   */
  readonly sources: Promise<Source[]>;

  /**
The tool calls that have been executed in the last step.

Resolved when the response is finished.
     */
  readonly toolCalls: Promise<ToolCallUnion<TOOLS>[]>;

  /**
The tool results that have been generated in the last step.

Resolved when the all tool executions are finished.
   */
  readonly toolResults: Promise<ToolResultUnion<TOOLS>[]>;

  /**
The reason why the generation finished. Taken from the last step.

Resolved when the response is finished.
     */
  readonly finishReason: Promise<FinishReason>;

  /**
The token usage of the last step.

Resolved when the response is finished.
   */
  readonly usage: Promise<LanguageModelUsage>;

  /**
The total token usage of the generated response.
When there are multiple steps, the usage is the sum of all step usages.

Resolved when the response is finished.
     */
  readonly totalUsage: Promise<LanguageModelUsage>;

  /**
Warnings from the model provider (e.g. unsupported settings) for the first step.
     */
  readonly warnings: Promise<CallWarning[] | undefined>;

  /**
Details for all steps.
You can use this to get information about intermediate steps,
such as the tool calls or the response headers.
   */
  readonly steps: Promise<Array<StepResult<TOOLS>>>;

  /**
Additional request information from the last step.
 */
  readonly request: Promise<LanguageModelRequestMetadata>;

  /**
Additional response information from the last step.
 */
  readonly response: Promise<
    LanguageModelResponseMetadata & {
      /**
The response messages that were generated during the call. It consists of an assistant message,
potentially containing tool calls.

When there are tool results, there is an additional tool message with the tool results that are available.
If there are tools that do not have execute functions, they are not included in the tool results and
need to be added separately.
       */
      messages: Array<ResponseMessage>;
    }
  >;

  /**
Additional provider-specific metadata from the last step.
Metadata is passed through from the provider to the AI SDK and
enables provider-specific results that can be fully encapsulated in the provider.
   */
  readonly providerMetadata: Promise<ProviderMetadata | undefined>;

  /**
  A text stream that returns only the generated text deltas. You can use it
  as either an AsyncIterable or a ReadableStream. When an error occurs, the
  stream will throw the error.
     */
  readonly textStream: AsyncIterableStream<string>;

  /**
  A stream with all events, including text deltas, tool calls, tool results, and
  errors.
  You can use it as either an AsyncIterable or a ReadableStream.
  Only errors that stop the stream, such as network errors, are thrown.
     */
  readonly fullStream: AsyncIterableStream<TextStreamPart<TOOLS>>;

  /**
A stream of partial outputs. It uses the `experimental_output` specification.
   */
  readonly experimental_partialOutputStream: AsyncIterableStream<PARTIAL_OUTPUT>;

  /**
Consumes the stream without processing the parts.
This is useful to force the stream to finish.
It effectively removes the backpressure and allows the stream to finish,
triggering the `onFinish` callback and the promise resolution.

If an error occurs, it is passed to the optional `onError` callback.
  */
  consumeStream(options?: ConsumeStreamOptions): Promise<void>;

  /**
  Converts the result to a UI message stream.

  @param options.getErrorMessage an optional function that converts an error to an error message.
  @param options.sendUsage whether to send the usage information to the client. Defaults to true.
  @param options.sendReasoning whether to send the reasoning information to the client. Defaults to false.
  @param options.sendSources whether to send the sources information to the client. Defaults to false.
  @param options.experimental_sendFinish whether to send the finish information to the client. Defaults to true.
  @param options.experimental_sendStart whether to send the start information to the client. Defaults to true.

  @return A UI message stream.
     */
  toUIMessageStream<UI_MESSAGE extends UIMessage>(
    options?: UIMessageStreamOptions<UI_MESSAGE>,
  ): ReadableStream<InferUIMessageStreamPart<UI_MESSAGE>>;

  /**
  Writes UI message stream output to a Node.js response-like object.
  @param response A Node.js response-like object (ServerResponse).
  @param options.status The status code.
  @param options.statusText The status text.
  @param options.headers The headers.
  @param options.getErrorMessage An optional function that converts an error to an error message.
  @param options.sendUsage Whether to send the usage information to the client. Defaults to true.
  @param options.sendReasoning Whether to send the reasoning information to the client. Defaults to false.
     */
  pipeUIMessageStreamToResponse<UI_MESSAGE extends UIMessage>(
    response: ServerResponse,
    options?: UIMessageStreamResponseInit & UIMessageStreamOptions<UI_MESSAGE>,
  ): void;

  /**
  Writes text delta output to a Node.js response-like object.
  It sets a `Content-Type` header to `text/plain; charset=utf-8` and
  writes each text delta as a separate chunk.
  @param response A Node.js response-like object (ServerResponse).
  @param init Optional headers, status code, and status text.
     */
  pipeTextStreamToResponse(response: ServerResponse, init?: ResponseInit): void;

  /**
  Converts the result to a streamed response object with a stream data part stream.

  @param options.status The status code.
  @param options.statusText The status text.
  @param options.headers The headers.
  @param options.getErrorMessage An optional function that converts an error to an error message.
  @param options.sendUsage Whether to send the usage information to the client. Defaults to true.
  @param options.sendReasoning Whether to send the reasoning information to the client. Defaults to false.
  @return A response object.
     */
  toUIMessageStreamResponse<UI_MESSAGE extends UIMessage>(
    options?: UIMessageStreamResponseInit & UIMessageStreamOptions<UI_MESSAGE>,
  ): Response;

  /**
  Creates a simple text stream response.
  Each text delta is encoded as UTF-8 and sent as a separate chunk.
  Non-text-delta events are ignored.
  @param init Optional headers, status code, and status text.
     */
  toTextStreamResponse(init?: ResponseInit): Response;
}

export type TextStreamPart<TOOLS extends ToolSet> =
  | ContentPart<TOOLS>
  | { type: 'reasoning-part-finish' }
  | {
      type: 'tool-call-streaming-start';
      toolCallId: string;
      toolName: string;
    }
  | {
      type: 'tool-call-delta';
      toolCallId: string;
      toolName: string;
      inputTextDelta: string;
    }
  | {
      type: 'start-step';
      request: LanguageModelRequestMetadata;
      warnings: CallWarning[];
    }
  | {
      type: 'finish-step';
      response: LanguageModelResponseMetadata;
      usage: LanguageModelUsage;
      finishReason: FinishReason;
      providerMetadata: ProviderMetadata | undefined;
    }
  | {
      type: 'start';
    }
  | {
      type: 'finish';
      finishReason: FinishReason;
      totalUsage: LanguageModelUsage;
    }
  | {
      type: 'error';
      error: unknown;
    }
  | {
      type: 'raw';
      rawValue: unknown;
    };
