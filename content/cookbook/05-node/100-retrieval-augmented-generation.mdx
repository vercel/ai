---
title: Retrieval Augmented Generation
description: Learn how to use retrieval augmented generation using the AI SDK and Node
tags: ['node']
---

# Retrieval Augmented Generation

Retrieval Augmented Generation (RAG) is a technique that enhances the capabilities of language models by providing them with relevant information from external sources during the generation process.
This approach allows the model to access and incorporate up-to-date or specific knowledge that may not be present in its original training data.

This example uses [the following essay](https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt) as an input (`essay.txt`). Below you'll find two implementations: one using a simple in-memory vector database, and another using [Upstash Search](https://upstash.com/docs/search) for persistent hybrid search. For a more in-depth guide, check out the [RAG Chatbot Guide](/docs/guides/rag-chatbot) which will show you how to build a RAG chatbot with [Next.js](https://nextjs.org).

```ts
import fs from 'fs';
import path from 'path';
import dotenv from 'dotenv';
import { openai } from '@ai-sdk/openai';
import { cosineSimilarity, embed, embedMany, generateText } from 'ai';

dotenv.config();

async function main() {
  const db: { embedding: number[]; value: string }[] = [];

  const essay = fs.readFileSync(path.join(__dirname, 'essay.txt'), 'utf8');
  const chunks = essay
    .split('.')
    .map(chunk => chunk.trim())
    .filter(chunk => chunk.length > 0 && chunk !== '\n');

  const { embeddings } = await embedMany({
    model: openai.textEmbeddingModel('text-embedding-3-small'),
    values: chunks,
  });
  embeddings.forEach((e, i) => {
    db.push({
      embedding: e,
      value: chunks[i],
    });
  });

  const input =
    'What were the two main things the author worked on before college?';

  const { embedding } = await embed({
    model: openai.textEmbeddingModel('text-embedding-3-small'),
    value: input,
  });
  const context = db
    .map(item => ({
      document: item,
      similarity: cosineSimilarity(embedding, item.embedding),
    }))
    .sort((a, b) => b.similarity - a.similarity)
    .slice(0, 3)
    .map(r => r.document.value)
    .join('\n');

  const { text } = await generateText({
    model: openai('gpt-4o'),
    prompt: `Answer the following question based only on the provided context:
             ${context}

             Question: ${input}`,
  });
  console.log(text);
}

main().catch(console.error);
```

## Using Upstash Search

For production applications, you'll want to persist your data in a searchable database. [Upstash Search](https://upstash.com/docs/search) provides a search solution that combines input enrichment, reranking, semantic search, full-text search for more accurate results:

```ts
import fs from 'fs';
import path from 'path';
import dotenv from 'dotenv';
import { openai } from '@ai-sdk/openai';
import { generateText } from 'ai';
import { Search } from '@upstash/search';

dotenv.config();

type ChunkContent = {
  text: string;
  section: string;
};

// Initialize Upstash Search client
const search = new Search({
  url: process.env.UPSTASH_SEARCH_REST_URL!,
  token: process.env.UPSTASH_SEARCH_REST_TOKEN!,
});

const index = search.index<ChunkContent>('essay-chunks');

async function main() {
  const essay = fs.readFileSync(path.join(__dirname, 'essay.txt'), 'utf8');
  const chunks = essay
    .split('.')
    .map(chunk => chunk.trim())
    .filter(chunk => chunk.length > 0 && chunk !== '\n');

  // Upsert chunks to Upstash Search
  await index.upsert(
    chunks.map((chunk, i) => ({
      id: `chunk-${i}`,
      content: {
        text: chunk,
        section: `section-${Math.floor(i / 10)}`, // Group chunks into sections
      },
    })),
  );

  const input =
    'What were the two main things the author worked on before college?';

  // Search for relevant context using hybrid search
  const results = await index.search({
    query: input,
    limit: 3,
    reranking: true,
  });

  const context = results.hits
    .map(hit => hit.data?.text)
    .filter(Boolean)
    .join('\n');

  const { text } = await generateText({
    model: openai('gpt-4o'),
    prompt: `Answer the following question based only on the provided context:
             ${context}

             Question: ${input}`,
  });

  console.log(text);
}

main().catch(console.error);
```
