---
title: countTokens
description: API Reference for countTokens.
---

# `countTokens()`

Count the number of tokens in a prompt before sending it to a language model.

This is useful for:

- **Cost estimation**: Know how much an API call will cost before making it
- **Context management**: Ensure prompts fit within model context limits
- **Prompt optimization**: Compare token counts across different prompt variations

```ts
import { anthropic } from '@ai-sdk/anthropic';
import { countTokens } from 'ai';

const { tokens } = await countTokens({
  model: anthropic('claude-sonnet-4-5-20250929'),
  messages: [{ role: 'user', content: 'Explain quantum computing.' }],
});

console.log(`This prompt uses ${tokens} tokens`);
```

<Note>
  Token counting is supported by: **Anthropic**, **Google AI**, **Vertex AI**,
  **Amazon Bedrock** (via native APIs), and **OpenAI**, **Azure OpenAI** (via
  local tiktoken estimation).

For OpenAI/Azure, counts are estimates using tiktoken and may differ slightly
from actual API usage. The result includes
`providerMetadata.openai.estimatedTokenCount: true` to indicate this.

</Note>

## Import

<Snippet text={`import { countTokens } from "ai"`} prompt={false} />

## API Signature

### Parameters

<PropertiesTable
  content={[
    {
      name: 'model',
      type: 'LanguageModel',
      description:
        "The language model to use for token counting. Example: anthropic('claude-sonnet-4-5-20250929')",
    },
    {
      name: 'prompt',
      type: 'string',
      isOptional: true,
      description:
        'A simple text prompt. Cannot be used together with messages.',
    },
    {
      name: 'system',
      type: 'string',
      isOptional: true,
      description: 'System message to include in the prompt.',
    },
    {
      name: 'messages',
      type: 'Array<ModelMessage>',
      isOptional: true,
      description:
        'Messages in the conversation. Cannot be used together with prompt.',
    },
    {
      name: 'tools',
      type: 'Record<string, Tool>',
      isOptional: true,
      description:
        'Tools available to the model. Tool definitions are included in token count.',
    },
    {
      name: 'maxRetries',
      type: 'number',
      isOptional: true,
      description:
        'Maximum number of retries. Set to 0 to disable retries. Default: 2.',
    },
    {
      name: 'abortSignal',
      type: 'AbortSignal',
      isOptional: true,
      description:
        'An optional abort signal that can be used to cancel the call.',
    },
    {
      name: 'headers',
      type: 'Record<string, string>',
      isOptional: true,
      description:
        'Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.',
    },
    {
      name: 'providerOptions',
      type: 'ProviderOptions',
      isOptional: true,
      description:
        'Provider-specific options that are passed through to the provider.',
    },
    {
      name: 'experimental_telemetry',
      type: 'TelemetrySettings',
      isOptional: true,
      description: 'Telemetry configuration. Experimental feature.',
      properties: [
        {
          type: 'TelemetrySettings',
          parameters: [
            {
              name: 'isEnabled',
              type: 'boolean',
              isOptional: true,
              description:
                'Enable or disable telemetry. Disabled by default while experimental.',
            },
            {
              name: 'recordInputs',
              type: 'boolean',
              isOptional: true,
              description:
                'Enable or disable input recording. Enabled by default.',
            },
            {
              name: 'recordOutputs',
              type: 'boolean',
              isOptional: true,
              description:
                'Enable or disable output recording. Enabled by default.',
            },
            {
              name: 'functionId',
              type: 'string',
              isOptional: true,
              description:
                'Identifier for this function. Used to group telemetry data by function.',
            },
            {
              name: 'metadata',
              isOptional: true,
              type: 'Record<string, string | number | boolean | Array<null | undefined | string> | Array<null | undefined | number> | Array<null | undefined | boolean>>',
              description:
                'Additional information to include in the telemetry data.',
            },
            {
              name: 'tracer',
              type: 'Tracer',
              isOptional: true,
              description: 'A custom tracer to use for the telemetry data.',
            },
          ],
        },
      ],
    },
  ]}
/>

### Returns

<PropertiesTable
  content={[
    {
      name: 'tokens',
      type: 'number',
      description: 'The total number of tokens in the prompt.',
    },
    {
      name: 'warnings',
      type: 'Warning[]',
      description:
        'Warnings from the model provider (e.g. unsupported settings).',
    },
    {
      name: 'response',
      type: 'Response',
      isOptional: true,
      description: 'Optional response data.',
      properties: [
        {
          type: 'Response',
          parameters: [
            {
              name: 'headers',
              isOptional: true,
              type: 'Record<string, string>',
              description: 'Response headers.',
            },
            {
              name: 'body',
              type: 'unknown',
              isOptional: true,
              description: 'The response body.',
            },
          ],
        },
      ],
    },
    {
      name: 'providerMetadata',
      type: 'ProviderMetadata | undefined',
      isOptional: true,
      description:
        'Optional metadata from the provider. For OpenAI, includes `estimatedTokenCount: true` to indicate the count is a local estimate rather than API-verified.',
    },
  ]}
/>

## Examples

### Basic Usage

```ts
import { anthropic } from '@ai-sdk/anthropic';
import { countTokens } from 'ai';

const result = await countTokens({
  model: anthropic('claude-sonnet-4-5-20250929'),
  messages: [{ role: 'user', content: 'Hello, how are you?' }],
});

console.log(`Token count: ${result.tokens}`);
```

### With System Message

```ts
const result = await countTokens({
  model: anthropic('claude-sonnet-4-5-20250929'),
  system: 'You are a helpful assistant.',
  messages: [{ role: 'user', content: 'What is the capital of France?' }],
});
```

### With Tools

```ts
import { anthropic } from '@ai-sdk/anthropic';
import { countTokens, tool } from 'ai';
import { z } from 'zod';

const result = await countTokens({
  model: anthropic('claude-sonnet-4-5-20250929'),
  messages: [{ role: 'user', content: 'What is the weather in Paris?' }],
  tools: {
    weather: tool({
      description: 'Get the current weather',
      inputSchema: z.object({
        location: z.string(),
      }),
    }),
  },
});
```

### With OpenAI (tiktoken estimation)

```ts
import { openai } from '@ai-sdk/openai';
import { countTokens } from 'ai';

const result = await countTokens({
  model: openai('gpt-4o'),
  messages: [{ role: 'user', content: 'Explain quantum computing.' }],
});

console.log(`Estimated tokens: ${result.tokens}`);
// Check if this was an estimate vs API-verified count
if (result.providerMetadata?.openai?.estimatedTokenCount) {
  console.log('Note: This is a tiktoken estimate');
}
```

### With Amazon Bedrock

```ts
import { bedrock } from '@ai-sdk/amazon-bedrock';
import { countTokens } from 'ai';

const result = await countTokens({
  model: bedrock('anthropic.claude-3-haiku-20240307-v1:0'),
  messages: [{ role: 'user', content: 'Explain serverless computing.' }],
});

console.log(`Token count: ${result.tokens}`);
```

### Error Handling for Unsupported Providers

```ts
import { UnsupportedFunctionalityError } from 'ai';

try {
  // Some providers may not support token counting
  await countTokens({
    model: someModel,
    messages: [{ role: 'user', content: 'Hello' }],
  });
} catch (error) {
  if (UnsupportedFunctionalityError.isInstance(error)) {
    console.log('Token counting not supported by this provider');
    // Fall back to estimation or use a supported provider
  }
}
```
