---
title: Telemetry
description: Using OpenTelemetry with AI SDK Core
---

# Telemetry

The Vercel AI SDK uses [OpenTelemetry](https://opentelemetry.io/) to collect telemetry data.
OpenTelemetry is an open-source observability framework designed to provide
standardized instrumentation for collecting telemetry data.

<Note type="warning">
  This feature is experimental and may change in the future. The following AI
  SDK functions support telemetry: `generateText`, `streamText`,
  `generateObject`, `streamObject`, `embed`, and `embedMany`.
</Note>

## Enabling telemetry

For Next.js applications, please follow the [Next.js OpenTelemetry guide](https://nextjs.org/docs/app/building-your-application/optimizing/open-telemetry) to enable telemetry first.

You can then use the `experimental_telemetry` option to enable telemetry on specific function calls while the feature is experimental:

```ts highlight="4"
const result = await generateText({
  model: openai('gpt-4-turbo'),
  prompt: 'Write a short story about a cat.',
  experimental_telemetry: { isEnabled: true },
});
```

When telemetry is enabled, you can also control if you want to record the input values and the output values for the function.
By default, both are enabled. You can disable them by setting the `recordInputs` and `recordOutputs` options to `false`.

Disabling the recording of inputs and outputs can be useful for privacy, data transfer, and performance reasons.
You might for example want to disable recording inputs if they contain sensitive information.

## Telemetry Metadata

You can provide a `functionId` to identify the function that the telemetry data is for,
and `metadata` to include additional information in the telemetry data.

```ts highlight="6-10"
const result = await generateText({
  model: openai('gpt-4-turbo'),
  prompt: 'Write a short story about a cat.',
  experimental_telemetry: {
    isEnabled: true,
    functionId: 'my-awesome-function',
    metadata: {
      something: 'custom',
      someOtherThing: 'other-value',
    },
  },
});
```

## Collected Data

### generateText function

`generateText` records 3 types of spans:

- `ai.generateText`: the full length of the generateText call. It contains 1 or more `ai.generateText.doGenerate` spans.
  It contains the [basic LLM span information](#basic-llm-span-information) and the following attributes:
  - `operation.name`: `ai.generateText` and the functionId that was set through `telemetry.functionId`
  - `ai.prompt`: the prompt that was used when calling `generateText`
  - `ai.result.text`: the text that was generated
  - `ai.result.toolCalls`: the tool calls that were made as part of the generation (stringified JSON)
  - `ai.finishReason`: the reason why the generation finished
  - `ai.settings.maxToolRoundtrips`: the maximum number of tool roundtrips that were set
- `ai.generateText.doGenerate`: a provider doGenerate call. It can contain `ai.toolCall` spans.
  It contains the [basic LLM span information](#basic-llm-span-information) and the following attributes:
  - `operation.name`: `ai.generateText.doGenerate` and the functionId that was set through `telemetry.functionId`
  - `ai.prompt.format`: the format of the prompt
  - `ai.prompt.messages`: the messages that were passed into the provider
  - `ai.result.text`: the text that was generated
  - `ai.result.toolCalls`: the tool calls that were made as part of the generation (stringified JSON)
  - `ai.finishReason`: the reason why the generation finished
  - standardized [gen_ai attributes](https://opentelemetry.io/docs/specs/semconv/gen-ai/llm-spans/)
    - `gen_ai.request.model`: the model that was used
    - `gen_ai.response.finish_reasons`: the finish reasons that were returned by the provider
    - `gen_ai.system`: the provider that was used
    - `gen_ai.usage.completion_tokens`: the number of completion tokens that were used
    - `gen_ai.usage.prompt_tokens`: the number of prompt tokens that were used
- `ai.toolCall`: a tool call that is made as part of the generateText call. See [Tool call spans](#tool-call-spans) for more details.

### streamText function

`streamText` records 3 types of spans:

- `ai.streamText`: the full length of the streamText call. It contains a `ai.streamText.doStream` span.
  It contains the [basic LLM span information](#basic-llm-span-information) and the following attributes:
  - `operation.name`: `ai.streamText` and the functionId that was set through `telemetry.functionId`
  - `ai.prompt`: the prompt that was used when calling `streamText`
  - `ai.result.text`: the text that was generated
  - `ai.result.toolCalls`: the tool calls that were made as part of the generation (stringified JSON)
  - `ai.finishReason`: the reason why the generation finished
- `ai.streamText.doStream`: a provider doStream call.
  This span contains an `ai.stream.firstChunk` event that is emitted when the first chunk of the stream is received.
  The `doStream` span can also contain `ai.toolCall` spans.
  It contains the [basic LLM span information](#basic-llm-span-information) and the following attributes:
  - `operation.name`: `ai.streamText.doStream` and the functionId that was set through `telemetry.functionId`
  - `ai.prompt.format`: the format of the prompt
  - `ai.prompt.messages`: the messages that were passed into the provider
  - `ai.result.text`: the text that was generated
  - `ai.result.toolCalls`: the tool calls that were made as part of the generation (stringified JSON)
  - `ai.finishReason`: the reason why the generation finished
  - standardized [gen_ai attributes](https://opentelemetry.io/docs/specs/semconv/gen-ai/llm-spans/)
    - `gen_ai.request.model`: the model that was used
    - `gen_ai.response.finish_reasons`: the finish reasons that were returned by the provider
    - `gen_ai.system`: the provider that was used
    - `gen_ai.usage.completion_tokens`: the number of completion tokens that were used
    - `gen_ai.usage.prompt_tokens`: the number of prompt tokens that were used
- `ai.toolCall`: a tool call that is made as part of the generateText call. See [Tool call spans](#tool-call-spans) for more details.

It also records a `ai.stream.firstChunk` event when the first chunk of the stream is received.

### generateObject function

`generateObject` records 2 types of spans:

- `ai.generateObject`: the full length of the generateObject call. It contains 1 or more `ai.generateObject.doGenerate` spans.
  It contains the [basic LLM span information](#basic-llm-span-information) and the following attributes:
  - `operation.name`: `ai.generateObject` and the functionId that was set through `telemetry.functionId`
  - `ai.prompt`: the prompt that was used when calling `generateObject`
  - `ai.schema`: Stringified JSON schema version of the schema that was passed into the `generateObject` function
  - `ai.result.object`: the object that was generated (stringified JSON)
  - `ai.settings.mode`: the object generation mode
- `ai.generateObject.doGenerate`: a provider doGenerate call.
  It contains the [basic LLM span information](#basic-llm-span-information) and the following attributes:
  - `operation.name`: `ai.generateObject.doGenerate` and the functionId that was set through `telemetry.functionId`
  - `ai.prompt.format`: the format of the prompt
  - `ai.prompt.messages`: the messages that were passed into the provider
  - `ai.result.object`: the object that was generated (stringified JSON)
  - `ai.settings.mode`: the object generation mode
  - `ai.finishReason`: the reason why the generation finished
  - standardized [gen_ai attributes](https://opentelemetry.io/docs/specs/semconv/gen-ai/llm-spans/)
    - `gen_ai.request.model`: the model that was used
    - `gen_ai.response.finish_reasons`: the finish reasons that were returned by the provider
    - `gen_ai.system`: the provider that was used
    - `gen_ai.usage.completion_tokens`: the number of completion tokens that were used
    - `gen_ai.usage.prompt_tokens`: the number of prompt tokens that were used

### streamObject function

`streamObject` records 2 types of spans:

- `ai.streamObject`: the full length of the streamObject call. It contains 1 or more `ai.streamObject.doStream` spans.
  It contains the [basic LLM span information](#basic-llm-span-information) and the following attributes:
  - `operation.name`: `ai.streamObject` and the functionId that was set through `telemetry.functionId`
  - `ai.prompt`: the prompt that was used when calling `streamObject`
  - `ai.schema`: Stringified JSON schema version of the schema that was passed into the `streamObject` function
  - `ai.result.object`: the object that was generated (stringified JSON)
  - `ai.settings.mode`: the object generation mode
- `ai.streamObject.doStream`: a provider doStream call.
  It contains the [basic LLM span information](#basic-llm-span-information) and the following attributes:
  - `operation.name`: `ai.streamObject.doStream` and the functionId that was set through `telemetry.functionId`
  - `ai.prompt.format`: the format of the prompt
  - `ai.prompt.messages`: the messages that were passed into the provider
  - `ai.result.object`: the object that was generated (stringified JSON)
  - `ai.settings.mode`: the object generation mode
  - `ai.finishReason`: the reason why the generation finished
  - standardized [gen_ai attributes](https://opentelemetry.io/docs/specs/semconv/gen-ai/llm-spans/)
    - `gen_ai.request.model`: the model that was used
    - `gen_ai.response.finish_reasons`: the finish reasons that were returned by the provider
    - `gen_ai.system`: the provider that was used
    - `gen_ai.usage.completion_tokens`: the number of completion tokens that were used
    - `gen_ai.usage.prompt_tokens`: the number of prompt tokens that were used

It also records a `ai.stream.firstChunk` event when the first chunk of the stream is received.

### embed function

`embed` records 2 types of spans:

- `ai.embed`: the full length of the embed call. It contains 1 `ai.embed.doEmbed` spans.
  It contains the [basic embedding span information](#basic-embedding-span-information) and the following attributes:
  - `operation.name`: `ai.embed` and the functionId that was set through `telemetry.functionId`
  - `ai.value`: the value that was passed into the `embed` function
  - `ai.embedding`: a JSON-stringified embedding
- `ai.embed.doEmbed`: a provider doEmbed call.
  It contains the [basic embedding span information](#basic-embedding-span-information) and the following attributes:
  - `operation.name`: `ai.embed.doEmbed` and the functionId that was set through `telemetry.functionId`
  - `ai.values`: the values that were passed into the provider (array)
  - `ai.embeddings`: an array of JSON-stringified embeddings

### embedMany function

`embedMany` records 2 types of spans:

- `ai.embedMany`: the full length of the embedMany call. It contains 1 or more `ai.embedMany.doEmbed` spans.
  It contains the [basic embedding span information](#basic-embedding-span-information) and the following attributes:
  - `operation.name`: `ai.embedMany` and the functionId that was set through `telemetry.functionId`
  - `ai.values`: the values that were passed into the `embedMany` function
  - `ai.embeddings`: an array of JSON-stringified embedding
- `ai.embedMany.doEmbed`: a provider doEmbed call.
  It contains the [basic embedding span information](#basic-embedding-span-information) and the following attributes:
  - `operation.name`: `ai.embedMany.doEmbed` and the functionId that was set through `telemetry.functionId`
  - `ai.values`: the values that were sent to the provider
  - `ai.embeddings`: an array of JSON-stringified embeddings for each value

## Span Details

### Basic LLM span information

Many spans that use LLMs (`ai.generateText`, `ai.generateText.doGenerate`, `ai.streamText`, `ai.streamText.doStream`,
`ai.generateObject`, `ai.generateObject.doGenerate`, `ai.streamObject`, `ai.streamObject.doStream`) contain the following attributes:

- `ai.model.id`: the id of the model
- `ai.model.provider`: the provider of the model
- `ai.request.headers.*`: the request headers that were passed in through `headers`
- `ai.settings.maxRetries`: the maximum number of retries that were set
- `ai.telemetry.functionId`: the functionId that was set through `telemetry.functionId`
- `ai.telemetry.metadata.*`: the metadata that was passed in through `telemetry.metadata`
- `ai.usage.completionTokens`: the number of completion tokens that were used
- `ai.usage.promptTokens`: the number of prompt tokens that were used
- `resource.name`: the functionId that was set through `telemetry.functionId`

### Basic embedding span information

Many spans that use embedding models (`ai.embed`, `ai.embed.doEmbed`, `ai.embedMany`, `ai.embedMany.doEmbed`) contain the following attributes:

- `ai.model.id`: the id of the model
- `ai.model.provider`: the provider of the model
- `ai.request.headers.*`: the request headers that were passed in through `headers`
- `ai.settings.maxRetries`: the maximum number of retries that were set
- `ai.telemetry.functionId`: the functionId that was set through `telemetry.functionId`
- `ai.telemetry.metadata.*`: the metadata that was passed in through `telemetry.metadata`
- `ai.usage.tokens`: the number of tokens that were used
- `resource.name`: the functionId that was set through `telemetry.functionId`

### Tool call spans

Tool call spans (`ai.toolCall`) contain the following attributes:

- `ai.toolCall.name`: the name of the tool
- `ai.toolCall.id`: the id of the tool call
- `ai.toolCall.args`: the parameters of the tool call
- `ai.toolCall.result`: the result of the tool call. Only available if the tool call is successful and the result is serializable.
