---
title: Hugging Face
description: Learn how to use Hugging Face Inference Providers with the AI SDK.
---

# Hugging Face Provider

The [Hugging Face Inference Providers](https://huggingface.co/inference/models) for the AI SDK contains language model support for hundreds of machine learning models, powered by world-class inference providers. Through a single API, you can access models from Cerebras, Groq, Together AI, Replicate, and more.

API keys can be obtained from your [Hugging Face user settings](https://huggingface.co/settings/tokens).

## Setup

The Hugging Face provider is available in the `@ai-sdk/huggingface` module. You can install it with

<Tabs items={['pnpm', 'npm', 'yarn']}>
  <Tab>
    <Snippet text="pnpm add @ai-sdk/huggingface" dark />
  </Tab>
  <Tab>
    <Snippet text="npm install @ai-sdk/huggingface" dark />
  </Tab>
  <Tab>
    <Snippet text="yarn add @ai-sdk/huggingface" dark />
  </Tab>
</Tabs>

## Provider Instance

You can import the default provider instance `huggingface` from `@ai-sdk/huggingface`:

```ts
import { huggingface } from '@ai-sdk/huggingface';
```

If you need a customized setup, you can import `createHuggingFace` from `@ai-sdk/huggingface`
and create a provider instance with your settings:

```ts
import { createHuggingFace } from '@ai-sdk/huggingface';

const huggingface = createHuggingFace({
  apiKey: process.env.HF_TOKEN ?? '',
});
```

You can use the following optional settings to customize the Hugging Face provider instance:

- **baseURL** _string_

  Use a different URL prefix for API calls, e.g. to use proxy servers.
  The default prefix is `https://router.huggingface.co/v1`.

- **apiKey** _string_

  API key that is being sent using the `Authorization` header.
  It defaults to the `HF_TOKEN` environment variable.

- **headers** _Record&lt;string,string&gt;_

  Custom headers to include in the requests.

- **fetch** _(input: RequestInfo, init?: RequestInit) => Promise&lt;Response&gt;_

  Custom [fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation.
  Defaults to the global `fetch` function.
  You can use it as a middleware to intercept requests,
  or to provide a custom fetch implementation for e.g. testing.

## Language Models

You can create [Hugging Face models](https://huggingface.co/inference/models) using a provider instance.
The first argument is the model id, e.g. `meta-llama/Llama-3.1-8B-Instruct`.

```ts
const model = huggingface('meta-llama/Llama-3.1-8B-Instruct');
```

You can also use the `.languageModel()` and `.chat()` methods:

```ts
const model = huggingface.languageModel('meta-llama/Llama-3.1-8B-Instruct');
const model = huggingface.chat('meta-llama/Llama-3.1-8B-Instruct');
```

### Example

You can use Hugging Face language models to generate text with the `generateText` function:

```ts
import { huggingface } from '@ai-sdk/huggingface';
import { generateText } from 'ai';

const { text } = await generateText({
  model: huggingface('meta-llama/Llama-3.1-8B-Instruct'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});
```

Hugging Face language models can also be used in the `streamText`, `generateObject`, and `streamObject` functions
(see [AI SDK Core](/docs/ai-sdk-core)).

### Provider Selection

Hugging Face Inference Providers automatically routes your request to the best available provider. You can also specify a provider explicitly using the `model:provider` format:

```ts
// Automatic provider selection
const model = huggingface('meta-llama/Llama-3.1-8B-Instruct');

// Explicit provider selection
const model = huggingface('meta-llama/Llama-3.1-8B-Instruct:sambanova');
```

Available providers include: `cerebras`, `cohere`, `fal`, `featherless`, `fireworks`, `groq`, `hyperbolic`, `nebius`, `novita`, `nscale`, `replicate`, `sambanova`, `together`.

### Vision-Language Models

Hugging Face supports vision-language models for multimodal tasks:

```ts
import { huggingface } from '@ai-sdk/huggingface';
import { generateText } from 'ai';

const { text } = await generateText({
  model: huggingface('Qwen/Qwen2.5-VL-7B-Instruct'),
  messages: [
    {
      role: 'user',
      content: [
        { type: 'text', text: 'Describe what you see in this image.' },
        { type: 'image', image: new URL('https://example.com/image.jpg') },
      ],
    },
  ],
});
```

## Model Capabilities

| Model                               | Text Generation     | Image Input         | Tool Usage          | Tool Streaming      |
| ----------------------------------- | ------------------- | ------------------- | ------------------- | ------------------- |
| `meta-llama/Llama-3.1-8B-Instruct`  | <Check size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |
| `meta-llama/Llama-3.1-70B-Instruct` | <Check size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |
| `deepseek-ai/DeepSeek-V3-0324`      | <Check size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |
| `Qwen/Qwen2.5-VL-7B-Instruct`       | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> | <Cross size={18} /> |
| `google/gemini-2.0-flash-exp`       | <Check size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |

<Note>
  Tool calling is not currently supported by Hugging Face's OpenAI-compatible
  endpoints. For the full list of available models and their capabilities, visit
  the [Hugging Face Inference Providers
  documentation](https://huggingface.co/inference/models).
</Note>
