---
title: LangChain
description: Learn how to use LangChain with the AI SDK.
---

# LangChain

[LangChain](https://docs.langchain.com/) is the platform for agent engineering.
It provides tools and abstractions for working with AI models, agents, vector stores,
and other data sources for retrieval augmented generation (RAG).

The `@ai-sdk/langchain` adapter provides seamless integration between LangChain and the AI SDK,
enabling you to use LangChain agents and graphs with AI SDK UI components.

## Installation

<Tabs items={['pnpm', 'npm', 'yarn']}>
  <Tab>
    <Snippet text="pnpm add @ai-sdk/langchain @langchain/core" dark />
  </Tab>
  <Tab>
    <Snippet text="npm install @ai-sdk/langchain @langchain/core" dark />
  </Tab>
  <Tab>
    <Snippet text="yarn add @ai-sdk/langchain @langchain/core" dark />
  </Tab>
</Tabs>

<Note>`@langchain/core` is a required peer dependency.</Note>

## Features

- Convert AI SDK `UIMessage` to LangChain `BaseMessage` format using `toBaseMessages`
- Transform LangChain/LangGraph streams to AI SDK `UIMessageStream` using `toUIMessageStream`
- `useLangSmithDeployment` transport for connecting directly to LangGraph deployments
- Full support for text, tool calls, tool results, and multimodal content

## Example: Basic Chat

Here is a basic example that uses both the AI SDK and LangChain together with the [Next.js](https://nextjs.org/docs) App Router.

```tsx filename="app/api/chat/route.ts"
import { toBaseMessages, toUIMessageStream } from '@ai-sdk/langchain';
import { ChatOpenAI } from '@langchain/openai';
import { createUIMessageStreamResponse, UIMessage } from 'ai';

export const maxDuration = 30;

export async function POST(req: Request) {
  const { messages }: { messages: UIMessage[] } = await req.json();

  const model = new ChatOpenAI({
    model: 'gpt-4o-mini',
    temperature: 0,
  });

  // Convert AI SDK UIMessages to LangChain messages
  const langchainMessages = await toBaseMessages(messages);

  // Stream the response from the model
  const stream = await model.stream(langchainMessages);

  // Convert the LangChain stream to UI message stream
  return createUIMessageStreamResponse({
    stream: toUIMessageStream(stream),
  });
}
```

Then, use the AI SDK's [`useChat`](/docs/ai-sdk-ui/chatbot) hook in the page component:

```tsx filename="app/page.tsx"
'use client';

import { useChat } from '@ai-sdk/react';

export default function Chat() {
  const { messages, sendMessage, status } = useChat();

  return (
    <div>
      {messages.map(m => (
        <div key={m.id}>
          {m.parts.map((part, i) =>
            part.type === 'text' ? <span key={i}>{part.text}</span> : null,
          )}
        </div>
      ))}
      <form
        onSubmit={e => {
          e.preventDefault();
          const input = e.currentTarget.elements.namedItem(
            'message',
          ) as HTMLInputElement;
          sendMessage({ text: input.value });
          input.value = '';
        }}
      >
        <input name="message" placeholder="Say something..." />
        <button type="submit" disabled={status === 'streaming'}>
          Send
        </button>
      </form>
    </div>
  );
}
```

## Example: LangChain Agent with Tools

Create agents with tools using LangChain's [`createAgent`](https://docs.langchain.com/oss/javascript/langchain/agents):

```tsx filename="app/api/agent/route.ts"
import { createUIMessageStreamResponse, UIMessage } from 'ai';
import { createAgent } from 'langchain';
import { ChatOpenAI, tools } from '@langchain/openai';
import { toBaseMessages, toUIMessageStream } from '@ai-sdk/langchain';

export const maxDuration = 60;

const model = new ChatOpenAI({
  model: 'gpt-4o',
  temperature: 0.7,
});

// Image generation tool configuration
const imageGenerationTool = tools.imageGeneration({
  size: '1024x1024',
  quality: 'high',
  outputFormat: 'png',
});

// Create a LangChain agent with tools
const agent = createAgent({
  model,
  tools: [imageGenerationTool],
  systemPrompt: 'You are a creative AI artist assistant.',
});

export async function POST(req: Request) {
  const { messages }: { messages: UIMessage[] } = await req.json();

  const langchainMessages = await toBaseMessages(messages);

  const stream = await agent.stream(
    { messages: langchainMessages },
    { streamMode: ['values', 'messages'] },
  );

  return createUIMessageStreamResponse({
    stream: toUIMessageStream(stream),
  });
}
```

## Example: LangGraph

Use the adapter with [LangGraph](https://docs.langchain.com/oss/javascript/langgraph/overview) to build agent workflows:

```tsx filename="app/api/langgraph/route.ts"
import { toBaseMessages, toUIMessageStream } from '@ai-sdk/langchain';
import { ChatOpenAI } from '@langchain/openai';
import { createUIMessageStreamResponse, UIMessage } from 'ai';
import { StateGraph, MessagesAnnotation } from '@langchain/langgraph';

export const maxDuration = 30;

const model = new ChatOpenAI({
  model: 'gpt-4o-mini',
  temperature: 0,
});

async function callModel(state: typeof MessagesAnnotation.State) {
  const response = await model.invoke(state.messages);
  return { messages: [response] };
}

export async function POST(req: Request) {
  const { messages }: { messages: UIMessage[] } = await req.json();

  // Create the LangGraph agent
  const graph = new StateGraph(MessagesAnnotation)
    .addNode('agent', callModel)
    .addEdge('__start__', 'agent')
    .addEdge('agent', '__end__')
    .compile();

  // Convert AI SDK UIMessages to LangChain messages
  const langchainMessages = await toBaseMessages(messages);

  // Stream from the graph using LangGraph's streaming format
  const stream = await graph.stream(
    { messages: langchainMessages },
    { streamMode: ['values', 'messages'] },
  );

  // Convert the LangGraph stream to UI message stream
  return createUIMessageStreamResponse({
    stream: toUIMessageStream(stream),
  });
}
```

## Example: LangSmith Deployment Transport

Connect directly to a LangGraph deployment from the browser using `useLangSmithDeployment`, bypassing the need for a backend API route:

```tsx filename="app/langsmith/page.tsx"
'use client';

import { useChat } from '@ai-sdk/react';
import { useLangSmithDeployment } from '@ai-sdk/langchain';

export default function LangSmithChat() {
  const { messages, sendMessage, status } = useChat({
    transport: useLangSmithDeployment({
      // Local development server
      url: 'http://localhost:2024',
      // Or for LangSmith deployment:
      // url: 'https://your-deployment.us.langgraph.app',
      // apiKey: process.env.NEXT_PUBLIC_LANGSMITH_API_KEY,
    }),
  });

  return (
    <div>
      {messages.map(m => (
        <div key={m.id}>
          {m.parts.map((part, i) =>
            part.type === 'text' ? <span key={i}>{part.text}</span> : null,
          )}
        </div>
      ))}
      <form
        onSubmit={e => {
          e.preventDefault();
          const input = e.currentTarget.elements.namedItem(
            'message',
          ) as HTMLInputElement;
          sendMessage({ text: input.value });
          input.value = '';
        }}
      >
        <input name="message" placeholder="Send a message..." />
        <button type="submit">Send</button>
      </form>
    </div>
  );
}
```

The `useLangSmithDeployment` function accepts the following options:

- `url`: The LangSmith deployment URL or local server URL (required)
- `apiKey`: API key for authentication (optional for local development)
- `graphId`: The ID of the graph to connect to (defaults to `'agent'`)

## API Reference

### `toBaseMessages(messages)`

Converts AI SDK `UIMessage` objects to LangChain `BaseMessage` objects.

```ts
import { toBaseMessages } from '@ai-sdk/langchain';

const langchainMessages = await toBaseMessages(uiMessages);
```

**Parameters:**

- `messages`: `UIMessage[]` - Array of AI SDK UI messages

**Returns:** `Promise<BaseMessage[]>`

### `convertModelMessages(modelMessages)`

Converts AI SDK `ModelMessage` objects to LangChain `BaseMessage` objects. Useful when you already have model messages from `convertToModelMessages`.

```ts
import { convertModelMessages } from '@ai-sdk/langchain';

const langchainMessages = convertModelMessages(modelMessages);
```

**Parameters:**

- `modelMessages`: `ModelMessage[]` - Array of model messages

**Returns:** `BaseMessage[]`

### `toUIMessageStream(stream)`

Converts a LangChain/LangGraph stream to an AI SDK `UIMessageStream`. Automatically detects the stream type and handles both direct model streams and LangGraph streams.

```ts
import { toUIMessageStream } from '@ai-sdk/langchain';
import { createUIMessageStreamResponse } from 'ai';

// Works with direct model streams
const modelStream = await model.stream(messages);
return createUIMessageStreamResponse({
  stream: toUIMessageStream(modelStream),
});

// Also works with LangGraph streams
const graphStream = await graph.stream(
  { messages },
  { streamMode: ['values', 'messages'] },
);
return createUIMessageStreamResponse({
  stream: toUIMessageStream(graphStream),
});
```

**Parameters:**

- `stream`: `AsyncIterable<AIMessageChunk> | ReadableStream` - LangChain or LangGraph stream

**Returns:** `ReadableStream<UIMessageChunk>`

### `useLangSmithDeployment(options)`

Creates a `ChatTransport` for LangSmith/LangGraph deployments. Use this with the `useChat` hook's `transport` option.

```ts
import { useLangSmithDeployment } from '@ai-sdk/langchain';
import { useChat } from '@ai-sdk/react';

const { messages, sendMessage } = useChat({
  transport: useLangSmithDeployment({
    url: 'https://your-deployment.us.langgraph.app',
    apiKey: 'your-api-key',
  }),
});
```

**Parameters:**

- `options`: `LangSmithDeploymentTransportOptions`
  - `url`: `string` - LangSmith deployment URL or local server URL (required)
  - `apiKey?`: `string` - API key for authentication (optional)
  - `graphId?`: `string` - The ID of the graph to connect to (defaults to `'agent'`)

**Returns:** `ChatTransport`

## More Examples

You can find additional examples in the AI SDK [examples/next-langchain](https://github.com/vercel/ai/tree/main/examples/next-langchain) folder.
