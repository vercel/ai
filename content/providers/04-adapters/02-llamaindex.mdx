---
title: LlamaIndex
description: Learn how to use LlamaIndex with the AI SDK.
---

# LlamaIndex

[LlamaIndex](https://ts.llamaindex.ai/) is a framework for building LLM-powered applications. LlamaIndex helps you ingest, structure, and access private or domain-specific data. LlamaIndex.TS offers the core features of LlamaIndex for Python for popular runtimes like Node.js (official support), Vercel Edge Functions (experimental), and Deno (experimental).

## Example:

Here is a basic example that uses both the AI SDK and LlamaIndex together with the [Next.js](https://nextjs.org/docs) App Router.

You can use the AI SDK's [`LlamaIndexWorkflowAdapter`](/docs/reference/stream-helpers/llamaindex-workflow-adapter) to stream events from a LlamaIndex [AgentWorkflow or Workflow](https://ts.llamaindex.ai/docs/llamaindex/modules/agents/agent_workflow) to the client.

```tsx filename="app/api/completion/route.ts" highlight="17"
import { tool, OpenAI } from 'llamaindex';
import { agent, startAgentEvent, run } from '@llamaindex/workflow';
import { LlamaIndexWorkflowAdapter } from 'ai';
import { z } from 'zod';

const calculatorAgent = agent({
  tools: [
    tool({
      name: 'add',
      description: 'Adds two numbers',
      parameters: z.object({ x: z.number(), y: z.number() }),
      execute: ({ x, y }) => x + y,
    }),
  ],
  llm: new OpenAI({ model: 'gpt-4o' }),
});

export async function POST(req: Request) {
  const { prompt } = await req.json();

  const eventStream = await run(
    calculatorAgent,
    startAgentEvent.with({
      userInput: prompt,
    }),
  );

  return LlamaIndexWorkflowAdapter.toDataStreamResponse(eventStream);
}
```

Alternatively, you can use the AI SDK's [`LlamaIndexAdapter`](/docs/reference/stream-helpers/llamaindex-adapter) to stream the result from calling the `chat` method on a [LlamaIndex ChatEngine](https://ts.llamaindex.ai/modules/chat_engine) or the `query` method on a [LlamaIndex QueryEngine](https://ts.llamaindex.ai/modules/query_engines), piping text directly to the client.

```tsx filename="app/api/completion/route.ts" highlight="17"
import { OpenAI, SimpleChatEngine } from 'llamaindex';
import { LlamaIndexAdapter } from 'ai';

export const maxDuration = 60;

export async function POST(req: Request) {
  const { prompt } = await req.json();

  const llm = new OpenAI({ model: 'gpt-4o' });
  const chatEngine = new SimpleChatEngine({ llm });

  const stream = await chatEngine.chat({
    message: prompt,
    stream: true,
  });

  return LlamaIndexAdapter.toDataStreamResponse(stream);
}
```

Finally, use the AI SDK's [`useCompletion`](/docs/ai-sdk-ui/completion) hook in your page component to handle completions:

```tsx filename="app/page.tsx"
'use client';

import { useCompletion } from '@ai-sdk/react';

export default function Chat() {
  const { completion, input, handleInputChange, handleSubmit } =
    useCompletion();

  return (
    <div>
      {completion}
      <form onSubmit={handleSubmit}>
        <input value={input} onChange={handleInputChange} />
      </form>
    </div>
  );
}
```

## More Examples

[create-llama](https://github.com/run-llama/create-llama) is the easiest way to get started with LlamaIndex. It uses the AI SDK to connect to LlamaIndex in all of its generated code.
