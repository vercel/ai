---
title: Portkey
description: Portkey Provider for the AI SDK
---

# Portkey Provider

[Portkey](https://portkey.ai) natively integrates with the Vercel AI SDK to make your apps production-ready and reliable. Import Portkey's Vercel package and use it as a provider in your Vercel AI app to enable all of Portkey's features:

- Full-stack **observability** and **tracing** for all requests
- Interoperability across **250+ LLMs**
- Built-in **50+** state-of-the-art guardrails
- Simple & semantic **caching** to save costs & time
- Conditional request routing with fallbacks, load-balancing, automatic retries, and more
- Continuous improvement based on user feedback

## Getting Started

### 1. Installation

```bash
npm install @portkey-ai/vercel-provider
```

### 2. Import & Configure Portkey Object

Sign up for Portkey, get your API key, and configure the Portkey provider in your Vercel app:

```typescript
import { createPortkey } from '@portkey-ai/vercel-provider';

const portkeyConfig = {
  provider: 'openai', // Choose your provider (e.g., 'anthropic', 'aws-bedrock')
  api_key: 'OPENAI_API_KEY',
  override_params: {
    model: 'gpt-4o',
  },
};

const portkey = createPortkey({
  apiKey: 'YOUR_PORTKEY_API_KEY',
  config: portkeyConfig,
});
```

Generate your API key in the [Portkey Dashboard](https://app.portkey.ai).

### 3. Running Your First Request

Portkey provider works with the `generateText` and `streamText` functions of the Vercel AI SDK.

#### Using `generateText`:

```typescript
import { createPortkey } from '@portkey-ai/vercel-provider';
import { generateText } from 'ai';

const portkeyConfig = {
  provider: 'openai',
  api_key: 'OPENAI_API_KEY',
  override_params: {
    model: 'gpt-4',
  },
};

const portkey = createPortkey({
  apiKey: 'YOUR_PORTKEY_API_KEY',
  config: portkeyConfig,
});

const { text } = await generateText({
  model: portkey.chatModel(''), // Provide an empty string, we defined the model in the config
  prompt: 'What is Portkey?',
});

console.log(text);
```

#### Using `streamText`:

```typescript
import { createPortkey } from '@portkey-ai/vercel-provider';
import { streamText } from 'ai';

const portkeyConfig = {
  provider: 'openai',
  api_key: 'OPENAI_API_KEY',
  override_params: {
    model: 'gpt-4',
  },
};

const portkey = createPortkey({
  apiKey: 'YOUR_PORTKEY_API_KEY',
  config: portkeyConfig,
});

const result = await streamText({
  model: portkey.completionModel(''), // Provide an empty string, we defined the model in the config
  prompt: 'Invent a new holiday and describe its traditions.',
});

for await (const chunk of result) {
  console.log(chunk);
}
```

## Portkey's `chatModel` & `completionModel`

Portkey supports `chatModel` and `completionModel` to easily handle chatbots or text completions. In the above examples, we used `portkey.chatModel` for generateText and `portkey.completionModel` for streamText.

## Tool Calling with Portkey

Portkey suppports Tool calling with Vercel AI SDK. Here's how-

```typescript
import { z } from 'zod';
import { generateText, tool } from 'ai';

const result = await generateText({
  model: portkey.chatModel('gpt-4-turbo'), // Model is an optional parameter; no need to redefine model if specified in config
  tools: {
    weather: tool({
      description: 'Get the weather in a location',
      parameters: z.object({
        location: z.string().describe('The location to get the weather for'),
      }),
      execute: async ({ location }) => ({
        location,
        temperature: 72 + Math.floor(Math.random() * 21) - 10,
      }),
    }),
  },
  prompt: 'What is the weather in San Francisco?',
});
```

# Portkey Features

Portkey helps make your Vercel app more robust and reliable. The Portkey config is a modular way to customize its functionality for your specific needs.

## Interoperability

Portkey allows you to easily switch between 250+ AI models by simply changing the model name in your configuration. This flexibility enables you to adapt to the evolving AI landscape without significant code changes.

### Switching from OpenAI to Anthropic

Here's how you'd use OpenAI with Portkey's Vercel integration:

```javascript
const portkeyConfig = {
  provider: 'openai',
  api_key: 'OPENAI_API_KEY',
  override_params: {
    model: 'gpt-4',
  },
};
```

To switch to Anthropic, change your provider slug to `anthropic` and enter your Anthropic API key along with the model of choice:

```javascript
const portkeyConfig = {
  provider: 'anthropic',
  api_key: 'Anthropic_API_KEY',
  override_params: {
    model: 'claude-3-sonnet-20240229',
  },
};
```

Here is is the list of (supported LLMs)[https://docs.portkey.ai/docs/integrations/llms]

## Observability

Portkey's OpenTelemetry-compliant observability suite gives you complete control over all your requests. Portkey's analytics dashboards provide 40+ key insights you're looking for, including cost, tokens, latency, and more.

![Portkey's Observability Dashboard](https://raw.githubusercontent.com/siddharthsambharia-portkey/Portkey-Product-Images/main/Portkey-Dashboard.png)

![Portkey's Logs](https://raw.githubusercontent.com/siddharthsambharia-portkey/Portkey-Product-Images/main/Portkey-Logs.png)

## Reliability

Portkey enhances the robustness of your AI applications with built-in features such as caching, fallback mechanisms, load balancing, conditional routing, and request timeouts.

Here's how you can modify your config to include the following Portkey features:

- Fallback
- Caching
- Conditional routing

```typescript
import { createPortkey } from '@portkey-ai/vercel-provider';
import { generateText } from 'ai';

const portkeyConfig = {
  strategy: {
    mode: 'fallback',
  },
  targets: [
    {
      provider: 'anthropic',
      api_key: 'Anthropic_API_KEY',
      override_params: {
        model: 'claude-3-sonnet-20240229',
      },
    },
    {
      provider: 'openai',
      api_key: 'OPENAI_API_KEY',
      override_params: {
        model: 'gpt-4o',
      },
    },
  ],
};

const portkey = createPortkey({
  apiKey: 'YOUR_PORTKEY_API_KEY',
  config: portkeyConfig,
});

const { text } = await generateText({
  model: portkey.chatModel(''),
  prompt: 'What is Portkey?',
});

console.log(text);
```

Learn more about Portkey's AI gateway features in detail [here](https://docs.portkey.ai/docs/product/ai-gateway/configs).

## Guardrails

Portkey Guardrails allow you to enforce LLM behavior in real-time, verifying both inputs and outputs against specified checks.

You can create Guardrail checks in the UI and then pass them in your Portkey Configs with before-request or after-request hooks.

Read more about Guardrails [here](https://docs.portkey.ai/docs/product/guardrails).

## Security and Compliance

Set budget limits on provider API keys and implement fine-grained user roles and permissions for both your application and the Portkey APIs.

## Advanced Configuration

The `portkeyConfig` object supports various options:

- `provider`: The AI provider to use (e.g., "openai", "anthropic")
- `api_key`: The API key for the chosen provider

- `strategy`: Defines routing and fallback strategies
- `targets`: Specifies multiple providers for fallback or load balancing
- `override_params`: Allows overriding default parameters- model, top_p, top_k, etc.

For a complete list of options, refer to the [Portkey documentation](https://docs.portkey.ai/docs/product/ai-gateway/configs).

## Additional Resources

- [üìò Portkey Documentation](https://docs.portkey.ai/docs/integrations/libraries/vercel)
- [üê¶ Twitter](https://twitter.com/portkeyai)
- [üí¨ Discord Community](https://discord.gg/JHPt4C7r)
- [üìä Portkey App](https://app.portkey.ai)
